{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from Transformer import *\n",
    "\n",
    "from PP import *\n",
    "import math\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Cityscapes\n",
    "from dataloader_cityscapes import *\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import functools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_last_layer(dim_in, stride = [1, 1], padding = [0, 0], dilation = [1, 1], kernel_size = [1, 1], output_padding = [0, 0]):\n",
    "\n",
    "    return ((dim_in + (2 * padding[0]) - (dilation[0] * (kernel_size[0] - 1)) - 1) /  stride[0]) + 1\n",
    "\n",
    "\n",
    "def choose_backbone():\n",
    "\n",
    "    torch.hub._validate_not_a_forked_repo=lambda a,b,c: True\n",
    "    backbone = torch.nn.Sequential(*(list(torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True).children())[:7]))\n",
    "    backbone.requires_grad = False\n",
    "    return backbone\n",
    "\n",
    "\n",
    "class customVariationalTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        super(customVariationalTransformer, self).__init__()\n",
    "\n",
    "        self.batch_size = kwargs[\"batch_size\"]\n",
    "        self.backbone = choose_backbone()\n",
    "\n",
    "        self.backbone_output_dim = functools.reduce(operator.mul, self.backbone(torch.rand(1, *(kwargs['prior_input_channels'], kwargs['input_img_dim'][0], kwargs['input_img_dim'][1])))).shape\n",
    "        self.seq_length = self.backbone_output_dim[0]\n",
    "\n",
    "        dim1 = prior_last_layer(self.backbone_output_dim[1])\n",
    "        dim2 = prior_last_layer(self.backbone_output_dim[2])\n",
    "        last_layer = int(dim1 * dim2)\n",
    "        layers = list(kwargs['prior_posterior_layers'])\n",
    "        layers.append(last_layer)\n",
    "\n",
    "\n",
    "        self.transformer = Transformer(d_model = last_layer, nhead = kwargs['transformer_num_heads'],\n",
    "                                        num_encoder_layers = kwargs['transformer_num_encoder_layer'], num_decoder_layers = kwargs['transformer_num_dec_layer'],\n",
    "                                        dim_feedforward = kwargs['transformer_intermediate_layer_dim'], dropout = kwargs['transformer_dropout_per'],\n",
    "                                        activation = \"relu\", return_intermediate_dec = False)\n",
    "\n",
    "        self.decoder_emb = nn.ConvTranspose2d(1, self.seq_length, kernel_size = 1, stride = 1)\n",
    "\n",
    "#         self.prior = AxisAlignedConvGaussian(input_channels = kwargs['prior_input_channels'], filters_enc = layers, inp_dim = kwargs['input_img_dim'])\n",
    "#         self.posterior = AxisAlignedConvGaussian(input_channels = kwargs['posterior_input_channels'], filters_enc = layers, inp_dim = kwargs['input_img_dim'])\n",
    "\n",
    "        self.prior = AxisAlignedConvGaussian(input_channels = kwargs['prior_input_channels'],\n",
    "                    num_filters = layers, no_convs_per_block = kwargs['pp_cnn_per_block'],\n",
    "                    latent_dim = kwargs['latent_dim']).to(device)\n",
    "        self.posterior = AxisAlignedConvGaussian(input_channels = kwargs['posterior_input_channels'],\n",
    "                        num_filters = layers, no_convs_per_block = kwargs['pp_cnn_per_block'],\n",
    "                        latent_dim = kwargs['latent_dim'], posterior=True).to(device)\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1, out_channels = kwargs[\"num_cat\"], kernel_size = 3, padding = 1, bias = True),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def inference(self, img):\n",
    "\n",
    "        prior_latent_space = self.prior.forward(img)\n",
    "        resnet_features = self.backbone(img)\n",
    "        transformer_encoder_output = self.transformer.encoder.forward(resnet_features.contiguous().view(img.shape[0], self.seq_length, -1))\n",
    "        for _ in range(16):\n",
    "            latent_vector_prior = self.sample(prior_latent_space, training = False)\n",
    "            decoder_embedding = self.decoder_emb(latent_vector_prior.unsqueeze(1).view(img.shape[0], 1, int(math.sqrt(latent_vector_prior.shape[1])), -1))\n",
    "            reconstruct_prior = self.transformer.decoder.forward(transformer_encoder_output, decoder_embedding.contiguous().view(img.shape[0], self.seq_length, -1))\n",
    "            reconstruct_prior = self.output_layer(reconstruct_prior.unsqueeze(1))\n",
    "            yield reconstruct_prior\n",
    "\n",
    "\n",
    "    def forward(self, img, segm):\n",
    "        \"\"\"\n",
    "        Construct prior latent space for patch and run patch through UNet,\n",
    "        in case training is True also construct posterior latent space\n",
    "        \"\"\"\n",
    "        prior_latent_space = self.prior.forward(img)\n",
    "        latent_vector_prior = self.sample(prior_latent_space, True)\n",
    "        #reconstruct_prior = self.transformer.forward(self.backbone(img), self.decoder_emb(latent_vector_prior))\n",
    "\n",
    "        posterior_latent_space = self.posterior.forward(img, segm)\n",
    "        latent_vector_posterior = self.sample(posterior_latent_space, True)\n",
    "\n",
    "\n",
    "        resnet_features = self.backbone(img)\n",
    "        decoder_embedding = self.decoder_emb(latent_vector_posterior.unsqueeze(1).view(self.batch_size,\n",
    "                                        1, int(math.sqrt(latent_vector_posterior.shape[1])), -1))\n",
    "#         print(resnet_features.shape)\n",
    "#         print(decoder_embedding.shape)\n",
    "#         print(self.seq_length)\n",
    "        reconstruct_posterior = self.transformer.forward(resnet_features.contiguous().view(self.batch_size,\n",
    "                                self.seq_length, -1), decoder_embedding.contiguous().view(self.batch_size,\n",
    "                                self.seq_length, -1))\n",
    "        #print(reconstruct_posterior.shape)\n",
    "        reconstruct_posterior = self.output_layer(reconstruct_posterior.unsqueeze(1))\n",
    "        #print(reconstruct_posterior.shape)\n",
    "\n",
    "        return prior_latent_space, posterior_latent_space, reconstruct_posterior\n",
    "\n",
    "\n",
    "    def sample(self, dist, training = False):\n",
    "        \"\"\"\n",
    "        Sample a segmentation by reconstructing from a prior sample\n",
    "        and combining this with UNet features\n",
    "        \"\"\"\n",
    "        if training == True:\n",
    "            z_prior = dist.rsample()\n",
    "        else:\n",
    "            z_prior = dist.sample()\n",
    "\n",
    "        return z_prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_in = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Resize((256,256))\n",
    "])\n",
    "\n",
    "preprocess_ou = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256,256))\n",
    "])\n",
    "\n",
    "tr_loader = CityscapesLoader(\"../datasets/augmented_cityscapes\", transform_in = preprocess_in, \n",
    "                             transform_ou = preprocess_ou)\n",
    "train_loader = DataLoader(dataset = tr_loader, batch_size = 5, shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/lunet/wsmo6/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/lunet/wsmo6/.conda/envs/3.7/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = customVariationalTransformer(**{\"input_img_dim\":[256,256],\n",
    "                       \"prior_input_channels\":3, \"prior_posterior_layers\":[64,128,256],\n",
    "                       \"posterior_input_channels\":37, \"batch_size\":5,\n",
    "                        \"transformer_num_heads\":2, \"transformer_num_encoder_layer\":2,\n",
    "                        \"transformer_num_dec_layer\":2,\"transformer_intermediate_layer_dim\":512,\n",
    "                        \"transformer_dropout_per\":0, \"num_cat\": 34, \"pp_cnn_per_block\":3,\n",
    "                        \"latent_dim\":256})\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(size_average = False, reduce = False, reduction = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstr:  tensor(0.7072, grad_fn=<MeanBackward0>)\n",
      "kl loss:  tensor(0.8387, grad_fn=<MeanBackward0>)\n",
      "elbo:  tensor(1.5460, grad_fn=<AddBackward0>)\n",
      "reconstr:  tensor(0.7071, grad_fn=<MeanBackward0>)\n",
      "kl loss:  tensor(0.7985, grad_fn=<MeanBackward0>)\n",
      "elbo:  tensor(1.5056, grad_fn=<AddBackward0>)\n",
      "reconstr:  tensor(0.7071, grad_fn=<MeanBackward0>)\n",
      "kl loss:  tensor(0.7560, grad_fn=<MeanBackward0>)\n",
      "elbo:  tensor(1.4630, grad_fn=<AddBackward0>)\n",
      "reconstr:  tensor(0.7072, grad_fn=<MeanBackward0>)\n",
      "kl loss:  tensor(0.7098, grad_fn=<MeanBackward0>)\n",
      "elbo:  tensor(1.4169, grad_fn=<AddBackward0>)\n",
      "reconstr:  tensor(0.7071, grad_fn=<MeanBackward0>)\n",
      "kl loss:  tensor(0.6563, grad_fn=<MeanBackward0>)\n",
      "elbo:  tensor(1.3634, grad_fn=<AddBackward0>)\n",
      "reconstr:  tensor(0.7071, grad_fn=<MeanBackward0>)\n",
      "kl loss:  tensor(0.5976, grad_fn=<MeanBackward0>)\n",
      "elbo:  tensor(1.3048, grad_fn=<AddBackward0>)\n",
      "reconstr:  tensor(0.7072, grad_fn=<MeanBackward0>)\n",
      "kl loss:  tensor(0.5399, grad_fn=<MeanBackward0>)\n",
      "elbo:  tensor(1.2471, grad_fn=<AddBackward0>)\n",
      "reconstr:  tensor(0.7071, grad_fn=<MeanBackward0>)\n",
      "kl loss:  tensor(0.5040, grad_fn=<MeanBackward0>)\n",
      "elbo:  tensor(1.2111, grad_fn=<AddBackward0>)\n",
      "reconstr:  tensor(0.7071, grad_fn=<MeanBackward0>)\n",
      "kl loss:  tensor(0.4670, grad_fn=<MeanBackward0>)\n",
      "elbo:  tensor(1.1741, grad_fn=<AddBackward0>)\n",
      "reconstr:  tensor(0.7071, grad_fn=<MeanBackward0>)\n",
      "kl loss:  tensor(0.4230, grad_fn=<MeanBackward0>)\n",
      "elbo:  tensor(1.1301, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d5283bb61b27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3.7/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3.7/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VT_modified/dataloader_cityscapes.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m#seg_color = transforms.ToTensor()(self.transform_ou(seg_color))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seg\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseg_color\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr =  0.0001, weight_decay = 0.)\n",
    "for batch in train_loader:\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    prior_latent_space, posterior_latent_space, reconstruct_posterior = model.forward(batch['image'].to(device), batch['label'].to(device))\n",
    "\n",
    "    kl_loss = torch.mean(kl.kl_divergence(posterior_latent_space, prior_latent_space))\n",
    "    reconstruction_loss = criterion(input = reconstruct_posterior, target = batch['label'])\n",
    "    reconstruction_loss = torch.mean(reconstruction_loss)\n",
    "\n",
    "    elbo = reconstruction_loss + (1. * kl_loss)\n",
    "    loss = elbo * 1.0\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"reconstr: \", reconstruction_loss)\n",
    "    print(\"kl loss: \", kl_loss)\n",
    "    print(\"elbo: \", elbo)\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.7",
   "language": "python",
   "name": "3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
