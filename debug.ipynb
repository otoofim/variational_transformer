{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d087547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from Transformer import *\n",
    "\n",
    "from PP import *\n",
    "import math\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Cityscapes\n",
    "from dataloader_cityscapes import *\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import functools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ceb3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_last_layer(dim_in, stride = [1, 1], padding = [0, 0], dilation = [1, 1], kernel_size = [1, 1], output_padding = [0, 0]):\n",
    "\n",
    "    return ((dim_in + (2 * padding[0]) - (dilation[0] * (kernel_size[0] - 1)) - 1) /  stride[0]) + 1\n",
    "\n",
    "\n",
    "def choose_backbone():\n",
    "\n",
    "    torch.hub._validate_not_a_forked_repo=lambda a,b,c: True\n",
    "    backbone = torch.nn.Sequential(*(list(torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True).children())[:7]))\n",
    "    backbone.requires_grad = False\n",
    "    return backbone\n",
    "\n",
    "\n",
    "class customVariationalTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        super(customVariationalTransformer, self).__init__()\n",
    "\n",
    "        self.batch_size = kwargs[\"batch_size\"]\n",
    "        self.backbone = choose_backbone()\n",
    "\n",
    "        self.backbone_output_dim = functools.reduce(operator.mul, self.backbone(torch.rand(1, *(kwargs['prior_input_channels'], kwargs['input_img_dim'][0], kwargs['input_img_dim'][1])))).shape\n",
    "        self.seq_length = self.backbone_output_dim[0]\n",
    "\n",
    "        dim1 = prior_last_layer(self.backbone_output_dim[1])\n",
    "        dim2 = prior_last_layer(self.backbone_output_dim[2])\n",
    "        last_layer = int(dim1 * dim2)\n",
    "        layers = list(kwargs['prior_posterior_layers'])\n",
    "        layers.append(last_layer)\n",
    "\n",
    "\n",
    "        self.transformer = Transformer(d_model = last_layer, nhead = kwargs['transformer_num_heads'],\n",
    "                                        num_encoder_layers = kwargs['transformer_num_encoder_layer'], num_decoder_layers = kwargs['transformer_num_dec_layer'],\n",
    "                                        dim_feedforward = kwargs['transformer_intermediate_layer_dim'], dropout = kwargs['transformer_dropout_per'],\n",
    "                                        activation = \"relu\", return_intermediate_dec = False)\n",
    "\n",
    "        self.decoder_emb = nn.ConvTranspose2d(1, self.seq_length, kernel_size = 1, stride = 1)\n",
    "\n",
    "#         self.prior = AxisAlignedConvGaussian(input_channels = kwargs['prior_input_channels'], filters_enc = layers, inp_dim = kwargs['input_img_dim'])\n",
    "#         self.posterior = AxisAlignedConvGaussian(input_channels = kwargs['posterior_input_channels'], filters_enc = layers, inp_dim = kwargs['input_img_dim'])\n",
    "\n",
    "        self.prior = AxisAlignedConvGaussian(input_channels = kwargs['prior_input_channels'],\n",
    "                    num_filters = layers, no_convs_per_block = kwargs['pp_cnn_per_block'],\n",
    "                    latent_dim = kwargs['latent_dim']).to(device)\n",
    "        self.posterior = AxisAlignedConvGaussian(input_channels = kwargs['posterior_input_channels'],\n",
    "                        num_filters = layers, no_convs_per_block = kwargs['pp_cnn_per_block'],\n",
    "                        latent_dim = kwargs['latent_dim'], posterior=True).to(device)\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1, out_channels = kwargs[\"num_cat\"], kernel_size = 3, padding = 1, bias = True),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def inference(self, img):\n",
    "\n",
    "        prior_latent_space = self.prior.forward(img)\n",
    "        resnet_features = self.backbone(img)\n",
    "        transformer_encoder_output = self.transformer.encoder.forward(resnet_features.contiguous().view(img.shape[0], self.seq_length, -1))\n",
    "        for _ in range(16):\n",
    "            latent_vector_prior = self.sample(prior_latent_space, training = False)\n",
    "            decoder_embedding = self.decoder_emb(latent_vector_prior.unsqueeze(1).view(img.shape[0], 1, int(math.sqrt(latent_vector_prior.shape[1])), -1))\n",
    "            reconstruct_prior = self.transformer.decoder.forward(transformer_encoder_output, decoder_embedding.contiguous().view(img.shape[0], self.seq_length, -1))\n",
    "            reconstruct_prior = self.output_layer(reconstruct_prior.unsqueeze(1))\n",
    "            yield reconstruct_prior\n",
    "\n",
    "\n",
    "    def forward(self, img, segm):\n",
    "        \"\"\"\n",
    "        Construct prior latent space for patch and run patch through UNet,\n",
    "        in case training is True also construct posterior latent space\n",
    "        \"\"\"\n",
    "        prior_latent_space = self.prior.forward(img)\n",
    "        latent_vector_prior = self.sample(prior_latent_space, True)\n",
    "        #reconstruct_prior = self.transformer.forward(self.backbone(img), self.decoder_emb(latent_vector_prior))\n",
    "\n",
    "        posterior_latent_space = self.posterior.forward(img, segm)\n",
    "        latent_vector_posterior = self.sample(posterior_latent_space, True)\n",
    "\n",
    "\n",
    "        resnet_features = self.backbone(img)\n",
    "        decoder_embedding = self.decoder_emb(latent_vector_posterior.unsqueeze(1).view(self.batch_size,\n",
    "                                        1, int(math.sqrt(latent_vector_posterior.shape[1])), -1))\n",
    "#         print(resnet_features.shape)\n",
    "#         print(decoder_embedding.shape)\n",
    "#         print(self.seq_length)\n",
    "        reconstruct_posterior = self.transformer.forward(resnet_features.contiguous().view(self.batch_size,\n",
    "                                self.seq_length, -1), decoder_embedding.contiguous().view(self.batch_size,\n",
    "                                self.seq_length, -1))\n",
    "        #print(reconstruct_posterior.shape)\n",
    "        reconstruct_posterior = self.output_layer(reconstruct_posterior.unsqueeze(1))\n",
    "        #print(reconstruct_posterior.shape)\n",
    "\n",
    "        return prior_latent_space, posterior_latent_space, reconstruct_posterior\n",
    "\n",
    "\n",
    "    def sample(self, dist, training = False):\n",
    "        \"\"\"\n",
    "        Sample a segmentation by reconstructing from a prior sample\n",
    "        and combining this with UNet features\n",
    "        \"\"\"\n",
    "        if training == True:\n",
    "            z_prior = dist.rsample()\n",
    "        else:\n",
    "            z_prior = dist.sample()\n",
    "\n",
    "        return z_prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254c826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_in = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Resize((256,256))\n",
    "])\n",
    "\n",
    "preprocess_ou = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256,256))\n",
    "])\n",
    "\n",
    "tr_loader = CityscapesLoader(\"../datasets/augmented_cityscapes\", transform_in = preprocess_in, \n",
    "                             transform_ou = preprocess_ou)\n",
    "train_loader = DataLoader(dataset = tr_loader, batch_size = 5, shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd5224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/lunet/wsmo6/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/lunet/wsmo6/anaconda3/envs/3.6/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1620716839079/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/lunet/wsmo6/anaconda3/envs/3.6/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = customVariationalTransformer(**{\"input_img_dim\":[256,256],\n",
    "                       \"prior_input_channels\":3, \"prior_posterior_layers\":[64,128,256],\n",
    "                       \"posterior_input_channels\":37, \"batch_size\":5,\n",
    "                        \"transformer_num_heads\":2, \"transformer_num_encoder_layer\":2,\n",
    "                        \"transformer_num_dec_layer\":2,\"transformer_intermediate_layer_dim\":512,\n",
    "                        \"transformer_dropout_per\":0, \"num_cat\": 34, \"pp_cnn_per_block\":3,\n",
    "                        \"latent_dim\":256})\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(size_average = False, reduce = False, reduction = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e85a93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstr:  tensor(0.7072, grad_fn=<MeanBackward0>)\n",
      "kl loss:  tensor(1.0669, grad_fn=<MeanBackward0>)\n",
      "elbo:  tensor(1.7741, grad_fn=<AddBackward0>)\n",
      "reconstr:  tensor(0.7072, grad_fn=<MeanBackward0>)\n",
      "kl loss:  tensor(1.0222, grad_fn=<MeanBackward0>)\n",
      "elbo:  tensor(1.7294, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d5283bb61b27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprior_latent_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_latent_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstruct_posterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposterior_latent_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_latent_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ff9849105925>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, segm)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0malso\u001b[0m \u001b[0mconstruct\u001b[0m \u001b[0mposterior\u001b[0m \u001b[0mlatent\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mprior_latent_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mlatent_vector_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_latent_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m#reconstruct_prior = self.transformer.forward(self.backbone(img), self.decoder_emb(latent_vector_prior))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/variational_transformer/PP.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, segm)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/variational_transformer/PP.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/3.6/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/3.6/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/3.6/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr =  0.0001, weight_decay = 0.)\n",
    "for batch in train_loader:\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    prior_latent_space, posterior_latent_space, reconstruct_posterior = model.forward(batch['image'].to(device), batch['label'].to(device))\n",
    "\n",
    "    kl_loss = torch.mean(kl.kl_divergence(posterior_latent_space, prior_latent_space))\n",
    "    reconstruction_loss = criterion(input = reconstruct_posterior, target = batch['label'])\n",
    "    reconstruction_loss = torch.mean(reconstruction_loss)\n",
    "\n",
    "    elbo = reconstruction_loss + (1. * kl_loss)\n",
    "    loss = elbo * 1.0\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"reconstr: \", reconstruction_loss)\n",
    "    print(\"kl loss: \", kl_loss)\n",
    "    print(\"elbo: \", elbo)\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ee922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c058954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6",
   "language": "python",
   "name": "3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
